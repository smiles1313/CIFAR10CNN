# -*- coding: utf-8 -*-
"""CNNs with CIFAR-10 (W24)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UqFyv5WNnM3ZfXBSCWDvmVIqUg9F61s3

# Convolutional neural networks with CIFAR-10

ACM AI Applied Track (Winter 2024)

CIFAR 10 is a very well known dataset for modern AI - it's a dataset of 60000 labeled images (50000 training and 10000 test images), with 10 categories of image (thus the 10 in the name) - airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck.  Each image is 32 by 32, and we will be using a convolutional neural network - an architecture slightly more complicated than the neural network we used for MNIST.

### Starting
To get started, we'll first be importing the libraries we are going to need and trying to switch over to GPU - this is just setting up our development environment
"""

# libraries we need
import torch # Pytorch
import numpy as np # Numpy - useful for data manipulation.
import matplotlib.pyplot as plt # Lets us display data
import torchvision # We use this for the dataset we'll use
# By importing torch we can already access these - this is just a convenience thing
import torch.nn as nn
import torch.nn.functional as F

from tqdm.notebook import tqdm # Lets us see loop progress

#An essential feauture of pytorch is its ability to utlize GPUs and TPUs
#This portion of the code chooses the device that while be used,
#if there is a GPU that uses the CUDA toolkit, then it will utlize it
#if not it will use the CPU for calculations
#Note many times, GPUs are much faster option, and allow for more realistic training times

if torch.cuda.is_available(): # Checks if CUDA is availiable, loads the device for computation to the GPU
    device = torch.device('cuda:0')
    print('Running on GPU')
    print(torch.cuda.get_device_name(0))
else:
    device = torch.device('cpu')
    print('Running on CPU')

"""Data Processing"""

# download the CIFAR 10 training and testing data sets from torchvision.datasets
# There are a set of datasets that are stored on server for pytorch, anyone can donwload them if they have pytorch set up
# They are split to training and test dataset, which do not intersect!
# If you were doing this with your own dataset, you would have to make your own dataset!
train_dataset = torchvision.datasets.CIFAR10(root='./cifar10', transform=torchvision.transforms.ToTensor(), download=True)
test_dataset = torchvision.datasets.CIFAR10(root='./cifar10', train=False, transform=torchvision.transforms.ToTensor(), download=True)

"""### Batching - an Aside
What the heck is 'batching'?  Why do we need it?  Well, remember how earlier we mentioned the 50000 training images - that's a lot of data!  Each one of those images has a size of 32 by 32 by 3 (it's a 32 by 32 image, so 32 by 32 pixels, and for each pixel we have an R, G, and B value) so if we were to try and run each image through our neural network, it would take a lot of memory and a long time!  So, we do what is called batching - dividing the dataset into a bunch of smaller subsets, running the subset through the neural network, and doing gradient descent on each one of these subsets instead of trying to do one huge dataset and one gradient descent at the very end.
"""

#The dataloader class makes it easy for us to handle and randomize data
#The train and test loader both have a 128 sized batches of images, and are shuffled to increase randomization(improves performance)
train_loader = torch.utils.data.DataLoader(train_dataset, 128, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, 128, shuffle=True)

# visualizing a sample from train loader
print(train_dataset)

# Extracting an image and label from the dataset
train_iter = iter(train_loader)
batch_images, batch_labels = next(train_iter)
# The label is a number between 0 and 9 - why might that be?  Isn't the label a category, not a number?
# The image is represented as a 3 by 32 by 32 matrix (A 3-dimensional array)
image, label = batch_images[0], batch_labels[0]

print(image.shape)
plt.imshow(image.permute(1, 2, 0)) # permute 1st 2nd 0th index is 32 32 3
plt.show()

"""### Making the CNN Class
We now need to define a CNN class for us to use
Remember with MNIST how we needed to define a class that inherits from the nn.Module class?  This is the same deal.  Whenever we want to train a neural network using pytorch, you need to define a new class that derives nn.Module.

But what exactly is a CNN?  If you remember with MNIST, we made a very basic neural network.  That can work, but a lot of the time we need to make slightly more complicated neural networks to acheive the results we want.  In a CNN, there's a couple of main layers that happen in a CNN to differentiate it from a normal NN that help it handle visual data

####Convolutional Layers:

<img src="https://miro.medium.com/v2/resize:fit:1358/1*ZCjPUFrB6eHPRi4eyP6aaA.gif" width=350/>

Let us apply a *convolution* to the input.  But what is a convolution?  We essentially slide a kernel (matrix) over the image and repeatedly take the dot product (that's a lot of math words).  But if you look at the gif above, hopefully it becomes more clear - the convolution is [[1,0,1],[0,1,0],[1,0,1]], so we just go through, multiplying each square in the area covered by our convolution by the corresponding square in the convolution and use that to build the convolved image.  The key insight here is that we won't be giving our CNN the convolutions - it will figure out what is most useful by itself to make it work well.  There's a couple more terms that are useful for us to know in relation to convolutions.
- Stride - this is how big the steps we are taking are.  In the above gif, you can tell that the stride is 1 - it moves over the image pixel by pixel.  But, if it were to have a stride of say 2, it would move not pixel by pixel but from one pixel to a pixel 2 away.  What would the size of the above convolution be if the stride was 2?  (It probably wouldn't be 3 by 3)
- Padding - if we're adding padding to the image we pass in, that's just adding a layer of pixels around the outside of the image.  This can be especially helpful if we want to apply the convolution to as much of the image as possible.  If you look at the gif above you might notice that the convolution is never centered on the outermost edge pixels.  But, if we were to add a 1 pixel layer of padding (a layer of extra pixels all around the outside), we would be able to center on the outermost edge pixels (as they are no longer the outermost edge).  How would a padding of 1 change the size of the convolution?
- Kernel Size - the size of a kernel.  The above kernel has a size of 3.  Kernels are squares so we only need one number to denote them.

#### Dropout

<img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/04/1IrdJ5PghD9YoOyVAQ73MJw.gif" width=350/>

One common problem in neural networks is overfitting - this is when your neural network gets really good at your training data, but not your testing data (Imagine I'm trying to study for a test and I only study 5 questions - I might learn the material, but it might be more likely that I just get *really good* at those 5 questions).

<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Overfitting.svg/300px-Overfitting.svg.png" width=350/>

*Overfitting example - see how we wanted a simple curve but got a really complicated one*

There's a number of factors that can cause overfitting, but more important is knowing how to counter it.  One common technique to counter overfitting is called dropout - this is when we randomly will make some neurons 'disappear'.  This helps combat overfitting because overfitting is usually characterized by an overly complicated function that is tailored to the data (see above image for an example).  But, if your neural network is unable to always have the full, super complicated function, that will probably cause it to fall apart.  The intuition behind this is pretty simple - if you are doing something really complicated, a small change can cause major disruption, but if what you are doing is simple, small changes will tend not to affect you.

#### Batch Norm

This is just normalization - when we look at data, it's probably helpful to normalize it.  Normalized data is easier for our neural networks to write generalized functions on, as this will make most of our data look relatively simple and easier to understand (imagine a pure blue or pure red image - if we don't normalize those images are vastly different, but after normalization we can easily tell that these images are in fact similar)

#### Relu

<img src="https://www.researchgate.net/publication/370465617/figure/fig2/AS:11431281155101569@1683057835840/Activation-function-ReLu-ReLu-Rectified-Linear-Activation.png" width=350/>

This is an activation function - you probably used this with MNIST.  It's essentially a straight line if x is > 0, and 0 otherwise.  This just tells us when to activate a neuron or not.  The reason why we use this has to do with vanishing gradients and other cool theory track concepts, but for us it's enough to just use it even if we don't have a perfect understanding of its nuances.

#### Pooling

<img src="https://production-media.paperswithcode.com/methods/MaxpoolSample2.png" width=350/>

This is a method we can use to scale down images.  One common method is max pooling (another common one is average but for our purposes we'll use max pooling).  This is essentially where we look at small squares of pixels and pick out one value - in our case, the maximum (hopefully you have a guess as to how average pooling works).  In the above image, they are halving the size by taking the max of 2 by 2 regions of pixels.  This differs from something like a convolution because there is no overlap.  What size region would we use if we wanted to make our images one third of their current size?

Neural Network documentation (I would especially recommend looking at Conv2d, BatchNorm2d, Dropout, MaxPool2d, and ReLu)
https://pytorch.org/docs/stable/nn.html

### Matrix Multiplication - a reminder

What we are doing here is a bunch of matrix multiplication!  Remember that with matrix multiplication, the first matrix must have the same number of columns as the second matrix has rows, and the resulting matrix has the same number of rows as the first matrix and the same number of columns as the second matrix (so a 2x4 matrix times a 4x7 matrix results in a 2x7 matrix).  This means that we need to make sure all our dimensions for our convolutions/other operations line up!
"""

#This is a class module for to create a CNN, not the Module class in pytorch
#Is the base class for all models in pytorch, this contains the inner working of a module

#Functions ->
# The def __init__(self) is a constructor, where you outline the different layers and aspects of your custom class
# def forward is the function for forward propogation you give it an input X and it outputs tensor

#Layers ->
#In pytorch a nn.Conv2d layer is a convolution 2d layer, the arguments are as follows
#nn.Conv2d(Number of Input features maps, Number of features maps, Kernel Size, Stride Size, Padding Size )
#nn.BatchNorm2d is a batch normalization layer that takes in a 2d tensor the argument is the number of input feature maps

class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()

        # First convolutional layer
        # Here we're defining a standard layer with Convolution, BatchNorm, and dropout
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1, stride=2)
                              #og, new dim
        # b x 3 x 32 x 32 -> b x 32 x 16 x 16, image divided by 2
        self.batchnorm1 = nn.BatchNorm2d(32)
        self.relu1 = nn.ReLU() # Using ReLU activation function
        self.dropout1 = nn.Dropout(0.1) # Adding dropout to prevent overfitting (recommend a rate of 0.1)

        # Second convolutional layer
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1, stride=2) # b x 32 x 16 x 16 -> b x 64 x 8 x 8
        self.batchnorm2 = nn.BatchNorm2d(64)
        self.relu2 = nn.ReLU()
        self.pool2 = nn.MaxPool2d(2, 2) # Adding a pooling layer to reduce spatial dimensions, b x 64 x 8 x 8 -> b x 64 x 4 x 4
        self.dropout2 = nn.Dropout(0.1) # Recommend rate of 0.05

        # Third convolutional layer
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1, stride=2) # b x 64 x 4 x 4 -> b x 64 x 4 x 4
        self.batchnorm3 = nn.BatchNorm2d(64)
        self.relu3 = nn.ReLU()
        self.dropout3 = nn.Dropout(0.05) # Recommend rate of 0.05
        self.flatten = nn.Flatten()  # b x 64 x 4 x 4 -> b x (64 * 4 * 4)

        # Fully connected layer - classifying the features into 10 classes
        #linear bc you flatten out results of other conv layers for classification
        self.fc = nn.Linear(64 * 4, 128) # 64 from the last conv layer, 10 for the number of classes, b x (64 * 4 * 4) -> b x 128
        self.relu4 = nn.ReLU()
        self.fc1 = nn.Linear(128, 10)  # b x 128 -> b x 10

    # This is already done - we're just calling the functions we define
    def forward(self, x):
        # Describing the forward pass through the network
        x = self.conv1(x)
        x = self.batchnorm1(x)
        x = self.relu1(x)
        x = self.dropout1(x)

        x = self.conv2(x)
        x = self.batchnorm2(x)
        x = self.relu2(x)
        x = self.pool2(x)
        x = self.dropout2(x)

        x = self.conv3(x)
        x = self.batchnorm3(x)
        x = self.relu3(x)
        x = self.dropout3(x)

        # After all those conv layers we can finally pass into a fully connected layer
        # Think about it like the neural network does a bunch of pre processing to make the image easier to understand before looking at it
        print(x.shape)  #for debugging
        x = self.flatten(x)  # Flattening the output of the conv layers for the fully connected layer
        print(x.shape)  #for debugging
        x = self.fc(x)
        x = self.relu4(x)
        x = self.fc1(x)
        return x  # The softmax (or another activation) can be implicitly applied by the loss function

input = torch.randn(32, 3, 32, 32)
model = CNN()
output = model(input)
print(output.shape)

# We are creating an instance of our CNN model, after which we load to model to
# the device either GPU or CPU
model = CNN()

model.to(device)

#Define our loss, in this case the loss is cross entorpy
#Loss is a number that tells our model how good it's doing
criterion = nn.CrossEntropyLoss()

#Define the optimizer here, the model.parameters() are all the parameters of our model, lr is the learning rate
optimizer = torch.optim.Adam(model.parameters(), lr=0.003, weight_decay=1e-5)

"""## Training

"""

#Training loop - will take the model, train loader, the optimizer and device
#Loops through each training data and trains the model
#Data is loaded in batches, not single instances
def train_one_epoch(model, train_loader, optimizer, criterion, device):
    model.train()
    for i, batch in tqdm(enumerate(train_loader)):  # looping through
        inputs, labels = batch # The batch contains the inputs and labels
        inputs = inputs.to(device)
        labels = labels.to(device)
        # Get the model output
        outputs = model(inputs)
        # Calculate the loss (using the criterion defined above)
        loss = criterion(outputs, labels) #run it thru the criterion defined before as cross entropy loss
        # Call loss.backward - this actually computes the gradients
        loss.backward()
        # Step forward with the optimizer and then zero out the gradients
        optimizer.step()
        optimizer.zero_grad() #zero out gradients
    print('End of epoch loss:', round(loss.item(), 3))

"""## Testing

"""

#Same as above, but no optimization, just testing for accruacy
@torch.no_grad() # Letting torch know we don't need the gradients as we are only testing
def test(model, test_loader, device):
    # Manually specified the classes - these are from the cifar-10 dataset
    classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']

    # Put the model in evaluation mode
    model.eval()
    correct = 0
    for i, batch in tqdm(enumerate(test_loader)):
         inputs, labels = batch
         inputs = inputs.to(device)
         labels = labels.to(device)
         # Get the output
         outputs = model(inputs)
         # Determine if it made the right prediction
         predictions = outputs.argmax(dim=1) #take max of predictions and max probability

         # If it made the right prediction, increase the number correct
         correct += (predictions == labels).sum().item() #??? what does this mean

    print(f"End of epoch accuracy: {100 * correct / len(test_dataset)}%")

    # visualizing the current model's performance
    for i in range(min(len(inputs), 8)):
        print('Guess:', classes[predictions[i]], '| Label:', classes[labels[i]])
        plt.imshow(inputs[i].cpu().permute(1,2,0))
        plt.show()

"""

# Running the train-test loop"""

#This is where the training and testing loop is called
NUM_EPOCHS = 2 # One epoch is one loop through the training data

for epoch in range(NUM_EPOCHS):
    print("Epoch: ", epoch + 1)
    train_one_epoch(model, train_loader, optimizer, criterion, device)
    test(model, test_loader, device)

size = 0
for param in model.parameters():
    size += np.prod(param.shape)
print(f"Number of parameters: {size}")

"""

## Saving the weights"""

torch.save(model.state_dict(), "model.pth")

# reload the weights just saved

model_new = CNN()
model_new.load_state_dict(torch.load("model.pth"))
model_new.to(device)
model_new.eval()

test(model_new, test_loader, device)